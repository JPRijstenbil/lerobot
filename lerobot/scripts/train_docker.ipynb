{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3346a8",
   "metadata": {},
   "source": [
    "## Installing Docker\n",
    "\n",
    "To install Docker, follow the instructions on the official Docker website: [Docker Installation Guide](https://docs.docker.com/get-docker/).\n",
    "\n",
    "### Building the Docker Image\n",
    "First make sure Docker is running. \n",
    "\n",
    "```bash\n",
    "docker build -t <image_name> -f <path_where_dockerfile_is_located>\\Dockerfile <lerobot_dir>\n",
    "```\n",
    "For example:\n",
    "\n",
    "```bash\n",
    "docker build -t lerobot_img -f docker\\lerobot-gpu\\Dockerfile C:\\Users\\jprij\\Documents\\lerobot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b80b7ef",
   "metadata": {},
   "source": [
    "## Compatibility with Windows WSL2\n",
    "\n",
    "Assuming\n",
    "- Graphics driver already installed\n",
    "- Windows 11\n",
    "- WSL is already installed (automatic with windows 11)\n",
    "If not, follow the instructions on the official Microsoft website: [WSL Installation Guide](https://docs.microsoft.com/en-us/windows/wsl/install).\n",
    "\n",
    "Follow step 3.Option1 from the Nvidia [website](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)\n",
    "Select the Ubuntu-wsl option and run the commands according to Nvidia's instructions.\n",
    "\n",
    "Check if the installation works, by running:\n",
    "```bash\n",
    "docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\n",
    "```\n",
    "\n",
    "this generates a docker container based on an Nvidia cuda image, runs Nvidia-smi, and then discards the container. \n",
    "\n",
    "The '--gpus all' option should be added because it tells docker to use the gpu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6bf35",
   "metadata": {},
   "source": [
    "## Run a container from the image\n",
    "\n",
    "```bash\n",
    "docker run -it --gpus all --shm-size=8g --name <container_name> <image_name> \n",
    "```\n",
    "\n",
    "Validate by running the following python code inside the container:\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "torch.cuda.is_available() # Should return 'True'\n",
    "\n",
    "device = torch.device('cuda')\n",
    "a = np.array([[1,2],[2,3]])\n",
    "b = torch.from_numpy(a)\n",
    "b.to(device) # Should return a cuda tensor\n",
    "print(b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c767dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Train a policy\n",
    "\n",
    "First log in to Hugginface and WandB\n",
    "\n",
    "```Python\n",
    "HF_USER=$(huggingface-cli whoami | head -n 1)\n",
    "```\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "wandb.login(key='WB_KEY')\n",
    "```\n",
    "\n",
    "Then run the training script, for example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247ab9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "HF_KEY = # Your Hugging Face API key\n",
    "WB_KEY = # Your Weights & Biases API key\n",
    "\n",
    "!huggingface-cli login --token HF_KEY\n",
    "\n",
    "HF_USER=$(huggingface-cli whoami | head -n 1)\n",
    "\n",
    "import wandb\n",
    "wandb.login(key='WB_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444f311",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run a training job with the specified parameters\n",
    "!python lerobot/scripts/train.py --dataset.repo_id=${HF_USER}/record-test-5 --policy.type=act --output_dir=outputs/train/act_so101_test_1 --job_name=act_so101_test_1 --policy.device=cuda --wandb.enable=true"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
